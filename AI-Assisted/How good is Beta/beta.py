"""
Speculative Stock Return Predictability Analysis
------------------------------------------------

THIS SCRIPT IS FOR EDUCATIONAL / EXPLORATORY USE ONLY.
The ticker lists it expects were generated by an AI model and **are NOT
historically accurate**.  They may contain omissions, incorrect symbols, or
entities that did not exist in a given year.  All data is fetched from
`yfinance`, which itself is not guaranteed to be point‑in‑time.

The code therefore emphasises *robust error handling* and *clear logging* so
that missing or unreliable data does not cause the entire run to fail.

Functionality
~~~~~~~~~~~~~
* Accepts a dict ``spec_yearly_tickers`` mapping *predictor year* (e.g. 2004)
  to a list of ticker symbols.
* For every (year, ticker) pair it
  1. downloads daily, **split‑ and dividend‑adjusted** prices (``auto_adjust=True``)
  2. computes predictor variables using data from year *t*:
     • CAPM beta vs **SPY**  (βᵢ,ₜ)
     • annualised volatility (Volᵢ,ₜ)
     • earnings‑to‑price (E/Pᵢ,ₜ, *best effort*)
     • dividend‑to‑price (D/Pᵢ,ₜ)
     • *optional* hindsight beta for year *t + 1* (βᵢ,ₜ₊₁)
  3. calculates total annualised return for year *t + 1* using adjusted closes
  4. records the results in a single ``pandas`` DataFrame.
* Saves ``speculative_predictability_dataset.csv``.
* Prints
  • Pearson correlations of each predictor with realised return
  • Simple univariate OLS regressions (statsmodels) of
    ``RealisedReturn_t1 ~ Predictor_t``.

How to use
~~~~~~~~~~
1. Replace the *placeholder* ``speculative_yearly_tickers`` dict at the bottom
   with the AI‑generated lists you posted (or import them from another file).
2. ``python speculative_stock_predictability.py``
3. Inspect the CSV and console output; iterate as you like.

Dependencies: ``pandas`` ≥ 1.5, ``numpy`` ≥ 1.23, ``yfinance`` ≥ 0.2,
``statsmodels`` ≥ 0.14, ``scipy`` ≥ 1.9.

-------------------------------------------------"""

import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy.stats import pearsonr
import yfinance as yf

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s:%(message)s"
)

# ---------------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------------

def get_daily_history(ticker: str, start: str, end: str) -> pd.DataFrame | None:
    """Download *adjusted* daily OHLCV data for ``ticker`` between ``start`` (inclusive)
    and ``end`` (exclusive).  Returns *None* on failure or if the DataFrame is empty."""
    try:
        data = yf.download(
            ticker,
            start=start,
            end=end,
            progress=False,
            auto_adjust=True,
            actions=False,
        )
        if data.empty:
            logging.warning("%s: no price data %s → %s", ticker, start, end)
            return None
        return data
    except Exception as exc:  # noqa: BLE001
        logging.warning("%s: download failed – %s", ticker, exc)
        return None


def compute_beta(r_stock: pd.Series, r_market: pd.Series) -> float:
    """CAPM beta = Cov(rᵢ, rₘ) / Var(rₘ).  NaN returned on insufficient data."""
    joined = pd.concat([r_stock, r_market], axis=1).dropna()
    if len(joined) < 50:  # fewer than ~10 weeks of trading days is too little
        return np.nan
    cov = np.cov(joined.iloc[:, 0], joined.iloc[:, 1])
    return cov[0, 1] / cov[1, 1] if cov[1, 1] != 0 else np.nan


def annualised_volatility(r: pd.Series) -> float:
    return r.std(ddof=0) * np.sqrt(252)


# ---------------------------------------------------------------------------
# Core per‑ticker computation
# ---------------------------------------------------------------------------

def metrics_for_year(
    ticker: str,
    year: int,
    spy_t: pd.DataFrame | None,
    spy_t1: pd.DataFrame | None,
) -> Dict[str, Any]:
    """Return a dict of all metrics for (*ticker*, *year*)."""
    out: Dict[str, Any] = {
        "Year": year,
        "Ticker": ticker,
        "Beta_t": np.nan,
        "Vol_t": np.nan,
        "EoverP_t": np.nan,
        "DoverP_t": np.nan,
        "Beta_t1": np.nan,
        "RealisedReturn_t1": np.nan,
    }

    # ------------------------------------------------------------------
    # Year t data
    # ------------------------------------------------------------------
    stock_t = get_daily_history(ticker, f"{year}-01-01", f"{year + 1}-01-01")
    if stock_t is None or stock_t.shape[0] < 200 or spy_t is None or spy_t.shape[0] < 200:
        return out  # leave NaNs; already logged

    r_stock_t = stock_t["Close"].pct_change().dropna()
    r_spy_t = spy_t["Close"].pct_change().dropna()

    # Align on common dates
    joined_idx = r_stock_t.index.intersection(r_spy_t.index)
    if len(joined_idx) >= 50:
        out["Beta_t"] = compute_beta(r_stock_t.loc[joined_idx], r_spy_t.loc[joined_idx])
    out["Vol_t"] = annualised_volatility(r_stock_t)

    price_year_end = stock_t["Close"].iloc[-1]

    # Earnings yield – best effort using current trailing EPS (not point‑in‑time!)
    try:
        eps_ttm = yf.Ticker(ticker).info.get("trailingEps", np.nan)
        out["EoverP_t"] = eps_ttm / price_year_end if np.isfinite(eps_ttm) else np.nan
    except Exception as exc:  # noqa: BLE001
        logging.debug("%s: EPS fetch failed – %s", ticker, exc)

    # Dividend yield
    try:
        div_series = yf.Ticker(ticker).dividends
        total_div = div_series.loc[
            (div_series.index >= f"{year}-01-01") & (div_series.index < f"{year + 1}-01-01")
        ].sum()
        out["DoverP_t"] = total_div / price_year_end if total_div else np.nan
    except Exception as exc:  # noqa: BLE001
        logging.debug("%s: dividend fetch failed – %s", ticker, exc)

    # ------------------------------------------------------------------
    # Year t + 1 for realised return & hindsight beta
    # ------------------------------------------------------------------
    if year + 1 <= datetime.now().year and spy_t1 is not None:
        stock_t1 = get_daily_history(ticker, f"{year + 1}-01-01", f"{year + 2}-01-01")
        if stock_t1 is not None and stock_t1.shape[0] >= 200:
            out["RealisedReturn_t1"] = (stock_t1["Close"].iloc[-1] / price_year_end) - 1.0

            r_stock_t1 = stock_t1["Close"].pct_change().dropna()
            r_spy_t1 = spy_t1["Close"].pct_change().dropna()
            j2 = r_stock_t1.index.intersection(r_spy_t1.index)
            if len(j2) >= 50:
                out["Beta_t1"] = compute_beta(r_stock_t1.loc[j2], r_spy_t1.loc[j2])

    return out


# ---------------------------------------------------------------------------
# Dataset builder & analysis
# ---------------------------------------------------------------------------

def build_dataset(spec_yearly_tickers: Dict[int, List[str]]) -> pd.DataFrame:
    records: List[Dict[str, Any]] = []
    spy_cache: Dict[int, pd.DataFrame | None] = {}

    for year, tickers in sorted(spec_yearly_tickers.items()):
        tickers_u = sorted({t.strip().upper() for t in tickers})
        logging.info("%s: %d tickers", year, len(tickers_u))

        # Cache SPY data for year t and t + 1 once each
        if year not in spy_cache:
            spy_cache[year] = get_daily_history("SPY", f"{year}-01-01", f"{year + 1}-01-01")
        if (year + 1) not in spy_cache and (year + 1) <= datetime.now().year:
            spy_cache[year + 1] = get_daily_history("SPY", f"{year + 1}-01-01", f"{year + 2}-01-01")

        spy_t = spy_cache.get(year)
        spy_t1 = spy_cache.get(year + 1)

        for tic in tickers_u:
            records.append(metrics_for_year(tic, year, spy_t, spy_t1))

    return pd.DataFrame(records)


def run_analysis(df: pd.DataFrame, predictors: List[str] | None = None) -> None:
    if predictors is None:
        predictors = [
            "Beta_t",
            "Vol_t",
            "EoverP_t",
            "DoverP_t",
            "Beta_t1",
        ]

    print("\n=== Pearson correlations with RealisedReturn_t1 ===")
    for p in predictors:
        d = df[[p, "RealisedReturn_t1"]].dropna()
        if len(d) < 30:
            print(f"{p:>10}: n = {len(d):>3} (skipped)")
            continue
        r, pv = pearsonr(d[p], d["RealisedReturn_t1"])
        print(f"{p:>10}: ρ = {r:+.3f}   p = {pv:.4f}   n = {len(d)}")

    print("\n=== Simple univariate OLS regressions ===")
    for p in predictors:
        d = df[[p, "RealisedReturn_t1"]].dropna()
        if len(d) < 30:
            print(f"\n{p}: skipped (insufficient data)")
            continue
        X = sm.add_constant(d[p])
        y = d["RealisedReturn_t1"]
        model = sm.OLS(y, X).fit()
        coef = model.params[p]
        pval = model.pvalues[p]
        r2 = model.rsquared
        print(
            f"\nRealisedReturn_t1 ~ {p}\n"
            f"  β (slope)   = {coef:+.4f}   p = {pval:.4f}\n"
            f"  R²          = {r2:.4f}   n = {len(d)}"
        )


# ---------------------------------------------------------------------------
# Run as script
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    
    speculative_yearly_tickers: Dict[int, List[str]] = {
    2004: [
        "GE", "XOM", "MSFT", "C", "PFE", "WMT", "AIG", "BAC", "INTC", "JNJ",
        "IBM", "CSCO", "PG", "CVX", "MRK", "KO", "PEP", "DELL", "AMGN", "HD",
        "MS", "GS", "WFC", "MO", "ORCL", "HPQ", "UNH", "MDT", "T", "CMCSA"
    ],
    2005: [
        "XOM", "GE", "MSFT", "C", "BAC", "PFE", "WMT", "AIG", "JNJ", "INTC",
        "PG", "IBM", "CVX", "CSCO", "GOOG", "MRK", "KO", "PEP", "HD", "AMGN",
        "WFC", "MS", "GS", "ORCL", "UNH", "T", "HPQ", "MDT", "DELL", "MO"
    ],
    2006: [
        "XOM", "GE", "MSFT", "C", "BAC", "T", "PG", "JNJ", "PFE", "WMT",
        "AIG", "CVX", "GOOG", "INTC", "IBM", "CSCO", "KO", "PEP", "HD", "MRK",
        "WFC", "AMGN", "MS", "GS", "ORCL", "UNH", "HPQ", "COP", "MDT", "SLB"
    ],
    2007: [
        "XOM", "GE", "MSFT", "T", "PG", "GOOG", "C", "JNJ", "BAC", "CVX",
        "WMT", "PFE", "IBM", "AAPL", "CSCO", "INTC", "AIG", "KO", "PEP", "WFC",
        "HD", "MRK", "ORCL", "MS", "GS", "UNH", "COP", "SLB", "HPQ", "MCD"
    ],
    2008: [
        "XOM", "MSFT", "WMT", "JNJ", "PG", "T", "CVX", "GOOG", "IBM", "PFE",
        "AAPL", "KO", "PEP", "MRK", "WFC", "JPM", "CSCO", "INTC", "GE", "BAC",
        "UNH", "ORCL", "MCD", "COP", "SLB", "HD", "AMGN", "HPQ", "C", "AIG"
    ],
    2009: [
        "XOM", "MSFT", "AAPL", "WMT", "JNJ", "PG", "GOOG", "CVX", "JPM", "T",
        "BRK-A", "BRK-B", "IBM", "PFE", "WFC", "INTC", "CSCO", "MRK", "KO", "PEP",
        "BAC", "ORCL", "GE", "UNH", "COP", "QCOM", "SLB", "AMZN", "DIS", "MCD", "HD" # Added HD to make it 30+
    ],
    2010: [
        "XOM", "AAPL", "MSFT", "CVX", "WMT", "BRK-A", "BRK-B", "PG", "GOOG", "JNJ",
        "JPM", "T", "IBM", "WFC", "PFE", "GE", "INTC", "KO", "ORCL", "CSCO",
        "MRK", "PEP", "BAC", "SLB", "UNH", "AMZN", "QCOM", "COP", "MCD", "DIS", "HD" # Added HD
    ],
    2011: [
        "AAPL", "XOM", "MSFT", "CVX", "IBM", "WMT", "BRK-A", "BRK-B", "GOOG", "PG",
        "JNJ", "T", "JPM", "WFC", "GE", "PFE", "INTC", "KO", "ORCL", "MRK",
        "PEP", "CSCO", "AMZN", "BAC", "SLB", "UNH", "QCOM", "MCD", "DIS", "COP", "HD" # Added HD
    ],
    2012: [
        "AAPL", "XOM", "MSFT", "GOOG", "BRK-A", "BRK-B", "WMT", "JNJ", "CVX", "GE",
        "PG", "IBM", "WFC", "JPM", "T", "PFE", "KO", "ORCL", "INTC", "MRK",
        "PEP", "AMZN", "CSCO", "BAC", "UNH", "QCOM", "MCD", "HD", "DIS", "SLB", "V" # Added V
    ],
    2013: [
        "AAPL", "XOM", "GOOG", "MSFT", "BRK-A", "BRK-B", "JNJ", "WMT", "GE", "WFC",
        "CVX", "PG", "JPM", "IBM", "PFE", "KO", "T", "ORCL", "MRK", "AMZN",
        "INTC", "PEP", "BAC", "CSCO", "UNH", "HD", "QCOM", "V", "MCD", "DIS", "FB" # Added FB
    ],
    2014: [
        "AAPL", "XOM", "MSFT", "GOOG", "GOOGL", "BRK-A", "BRK-B", "JNJ", "WFC", "WMT",
        "GE", "CVX", "JPM", "PG", "PFE", "FB", "AMZN", "BAC", "T", "INTC",
        "KO", "VZ", "MRK", "CSCO", "HD", "DIS", "PEP", "GILD", "ORCL", "CMCSA", "UNH", "V" # Added UNH, V
    ],
    2015: [
        "AAPL", "GOOG", "GOOGL", "MSFT", "XOM", "BRK-A", "BRK-B", "AMZN", "FB", "JNJ",
        "GE", "WFC", "WMT", "JPM", "PG", "PFE", "T", "CVX", "BAC", "V",
        "KO", "HD", "MRK", "INTC", "DIS", "VZ", "ORCL", "CSCO", "PEP", "UNH", "GILD", "CMCSA" # Added GILD, CMCSA
    ],
    2016: [
        "AAPL", "GOOG", "GOOGL", "MSFT", "BRK-A", "BRK-B", "AMZN", "XOM", "FB", "JNJ",
        "JPM", "WFC", "GE", "T", "PG", "BAC", "WMT", "CVX", "V", "PFE",
        "KO", "HD", "INTC", "MRK", "VZ", "DIS", "ORCL", "CSCO", "UNH", "PEP", "CMCSA", "MA" # Added CMCSA, MA
    ],
    2017: [
        "AAPL", "GOOG", "GOOGL", "MSFT", "AMZN", "FB", "BRK-A", "BRK-B", "JNJ", "JPM",
        "XOM", "BAC", "T", "WFC", "V", "WMT", "PG", "INTC", "HD", "CVX",
        "UNH", "PFE", "KO", "MRK", "CSCO", "DIS", "ORCL", "PEP", "MA", "CMCSA", "BA", "GE" # Added BA, GE
    ],
    2018: [
        "MSFT", "AAPL", "AMZN", "GOOG", "GOOGL", "BRK-A", "BRK-B", "FB", "JNJ", "JPM",
        "V", "XOM", "WMT", "BAC", "PG", "UNH", "T", "HD", "INTC", "PFE",
        "MRK", "MA", "CVX", "KO", "CSCO", "DIS", "BA", "WFC", "PEP", "ORCL", "NFLX", "CMCSA" # Added NFLX, CMCSA
    ],
    2019: [
        "AAPL", "MSFT", "AMZN", "GOOG", "GOOGL", "FB", "BRK-A", "BRK-B", "JNJ", "V",
        "JPM", "WMT", "PG", "MA", "XOM", "T", "BAC", "HD", "INTC", "UNH",
        "VZ", "MRK", "KO", "DIS", "CVX", "PFE", "PEP", "CSCO", "NFLX", "ADBE", "MCD", "CRM" # Added MCD, CRM
    ],
    2020: [
        "AAPL", "MSFT", "AMZN", "GOOG", "GOOGL", "FB", "TSLA", "BRK-A", "BRK-B", "V",
        "JNJ", "WMT", "JPM", "MA", "PG", "UNH", "NVDA", "HD", "BAC", "DIS",
        "VZ", "PYPL", "CRM", "ADBE", "NFLX", "KO", "MRK", "PFE", "INTC", "T", "PEP", "CMCSA" # Added PEP, CMCSA
    ],
    2021: [
        "AAPL", "MSFT", "GOOG", "GOOGL", "AMZN", "TSLA", "META", "NVDA", "BRK-A", "BRK-B",
        "V", "JNJ", "JPM", "UNH", "WMT", "HD", "PG", "MA", "BAC", "AVGO",
        "PFE", "CRM", "ADBE", "KO", "XOM", "PEP", "NFLX", "CSCO", "MRK", "DIS", "ORCL", "COST" # Added ORCL, COST
    ],
    2022: [
        "AAPL", "MSFT", "GOOG", "GOOGL", "AMZN", "BRK-A", "BRK-B", "NVDA", "TSLA", "META",
        "JNJ", "UNH", "XOM", "V", "JPM", "WMT", "PG", "HD", "MA", "LLY",
        "CVX", "PFE", "MRK", "KO", "BAC", "PEP", "AVGO", "ORCL", "COST", "ABBV", "CRM", "ADBE" # Added CRM, ADBE
    ],
    2023: [
        "AAPL", "MSFT", "GOOG", "GOOGL", "AMZN", "NVDA", "META", "BRK-A", "BRK-B", "TSLA",
        "LLY", "V", "UNH", "JPM", "XOM", "JNJ", "WMT", "AVGO", "PG", "MA",
        "HD", "ORCL", "MRK", "CVX", "KO", "COST", "ABBV", "PEP", "CRM", "ADBE", "BAC", "NFLX" # Added BAC, NFLX
    ],
    2024: [ # Based on my previous speculative list for 2024
        "MSFT", "AAPL", "NVDA", "GOOG", "GOOGL", "AMZN", "META", "LLY", "BRK-A", "BRK-B", # Assuming A & B for BRK
        "AVGO", "TSLA", "V", "JPM", "XOM", "UNH", "WMT", "JNJ", "MA", "PG",
        "HD", "ORCL", "MRK", "COST", "CVX", "CRM", "ABBV", "ADBE", "KO", "BAC", "PEP", "NFLX" # Adjusted for 30+
    ]
}

    df_all = build_dataset(speculative_yearly_tickers)

    # Save dataset
    out_path = Path("speculative_predictability_dataset.csv")
    df_all.to_csv(out_path, index=False)
    logging.info("Dataset saved → %s (%d rows)", out_path.resolve(), len(df_all))

    # Basic analysis
    run_analysis(df_all)
